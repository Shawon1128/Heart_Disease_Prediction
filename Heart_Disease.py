# -*- coding: utf-8 -*-
"""Data_Preprosessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jiNPilDkmPCUOebW-eojhPNhlQQpwVrk
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from scipy.stats import boxcox
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# %matplotlib inline

# Set the resolution of the plotted figures
plt.rcParams['figure.dpi'] = 200

# Configure Seaborn plot styles: Set background color and use dark grid
sns.set(rc={'axes.facecolor': '#faded9'}, style='darkgrid')

# Read dataset
df = pd.read_csv('/content/drive/MyDrive/Thesis/heart.csv')
df

"""Dataset Overview"""

# Display a concise summary of the dataframe
df.info()

"""Based on the data types and the feature explanations we had earlier, we can see that 9 columns (sex, cp, fbs, restecg, exang, slope, ca, thal, and target) are indeed numerical in terms of data type, but categorical in terms of their semantics. These features should be converted to string (object) data type for proper analysis and interpretation."""

# Define the continuous features
continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

# Identify the features to be converted to object data type
features_to_convert = [feature for feature in df.columns if feature not in continuous_features]

# Convert the identified features to object data type
df[features_to_convert] = df[features_to_convert].astype('object')

df.dtypes

"""**Summary Statistics for Numerical Variables and categorical Variables**"""

# Get the summary statistics for numerical variables
df.describe().T

# Get the summary statistics for categorical variables
df.describe(include='object')

"""Exploratory Data Analysis (EDA)
1. Univariate Analysis
2. Bivariate Analysis

1.Univariate Analysis

We undertake univariate analysis on the dataset's features, based on their datatype:

For continuous data: We employ histograms to gain insight into the distribution of each feature. This allows us to understand the central tendency, spread, and shape of the dataset's distribution.

For categorical data: Bar plots are utilized to visualize the frequency of each category. This provides a clear representation of the prominence of each category within the respective feature.

Numerical Variables Univariate Analysis

age', 'trestbps', 'chol', 'thalach', 'oldpeak
"""

# Filter out continuous features for the univariate analysis
df_continuous = df[continuous_features]

# Set up the subplot
fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# Loop to plot histograms for each continuous feature
for i, col in enumerate(df_continuous.columns):
    x = i // 3
    y = i % 3
    values, bin_edges = np.histogram(df_continuous[col],
                                     range=(np.floor(df_continuous[col].min()), np.ceil(df_continuous[col].max())))

    graph = sns.histplot(data=df_continuous, x=col, bins=bin_edges, kde=True, ax=ax[x, y],
                         edgecolor='none', color='red', alpha=0.6, line_kws={'lw': 3})
    ax[x, y].set_xlabel(col, fontsize=15)
    ax[x, y].set_ylabel('Count', fontsize=12)
    ax[x, y].set_xticks(np.round(bin_edges, 1))
    ax[x, y].set_xticklabels(ax[x, y].get_xticks(), rotation=45)
    ax[x, y].grid(color='lightgrey')

    for j, p in enumerate(graph.patches):
        ax[x, y].annotate('{}'.format(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height() + 1),
                          ha='center', fontsize=10, fontweight="bold")

    textstr = '\n'.join((
        r'$\mu=%.2f$' % df_continuous[col].mean(),
        r'$\sigma=%.2f$' % df_continuous[col].std()
    ))
    ax[x, y].text(0.75, 0.9, textstr, transform=ax[x, y].transAxes, fontsize=12, verticalalignment='top',
                  color='white', bbox=dict(boxstyle='round', facecolor='#ff826e', edgecolor='white', pad=0.5))

ax[1,2].axis('off')
plt.suptitle('Distribution of Continuous Variables', fontsize=20)
plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()

"""Categorical Variables Univariate Analysis"""

# Filter out categorical features for the univariate analysis
categorical_features = df.columns.difference(continuous_features)
df_categorical = df[categorical_features]

# Set up the subplot for a 4x2 layout
fig, ax = plt.subplots(nrows=5, ncols=2, figsize=(15, 18))

# Loop to plot bar charts for each categorical feature in the 4x2 layout
for i, col in enumerate(categorical_features):
    row = i // 2
    col_idx = i % 2

    # Calculate frequency percentages
    value_counts = df[col].value_counts(normalize=True).mul(100).sort_values()

    # Plot bar chart
    value_counts.plot(kind='barh', ax=ax[row, col_idx], width=0.8, color='red')

    # Add frequency percentages to the bars
    for index, value in enumerate(value_counts):
        ax[row, col_idx].text(value, index, str(round(value, 1)) + '%', fontsize=15, weight='bold', va='center')

    ax[row, col_idx].set_xlim([0, 95])
    ax[row, col_idx].set_xlabel('Frequency Percentage', fontsize=12)
    ax[row, col_idx].set_title(f'{col}', fontsize=20)

ax[4,1].axis('off')
plt.suptitle('Distribution of Categorical Variables', fontsize=22)
plt.tight_layout()
plt.subplots_adjust(top=0.95)
plt.show()

"""2. Bivariate Analysis

For our bivariate analysis on the dataset's features with respect to the target variable:

For continuous data: use bar plots to showcase the average value of each feature for the different target classes, and KDE plots to understand the distribution of each feature across the target classes. This aids in discerning how each feature varies between the two target outcomes.

For categorical data: employ 100% stacked bar plots to depict the proportion of each category across the target classes. This offers a comprehensive view of how different categories within a feature relate to the target.

Numerical Features vs Target

Bar plots - showing the mean values.

KDE plots - displaying the distribution for each target category.
"""

# Set color palette
sns.set_palette(['#ff826e', 'red'])

# Create the subplots
fig, ax = plt.subplots(len(continuous_features), 2, figsize=(15,15), gridspec_kw={'width_ratios': [1, 2]})

# Loop through each continuous feature to create barplots and kde plots
for i, col in enumerate(continuous_features):
    # Barplot showing the mean value of the feature for each target category
    graph = sns.barplot(data=df, x="target", y=col, ax=ax[i,0])

    # KDE plot showing the distribution of the feature for each target category
    sns.kdeplot(data=df[df["target"]==0], x=col, fill=True, linewidth=2, ax=ax[i,1], label='0')
    sns.kdeplot(data=df[df["target"]==1], x=col, fill=True, linewidth=2, ax=ax[i,1], label='1')
    ax[i,1].set_yticks([])
    ax[i,1].legend(title='Heart Disease', loc='upper right')

    # Add mean values to the barplot
    for cont in graph.containers:
        graph.bar_label(cont, fmt='         %.3g')

# Set the title for the entire figure
plt.suptitle('Continuous Features vs Target Distribution', fontsize=22)
plt.tight_layout()
plt.show()

"""Categorical Features vs Target"""

# Remove 'target' from the categorical_features
categorical_features = [feature for feature in categorical_features if feature != 'target']

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(15,10))

for i,col in enumerate(categorical_features):

    # Create a cross tabulation showing the proportion of purchased and non-purchased loans for each category of the feature
    cross_tab = pd.crosstab(index=df[col], columns=df['target'])

    # Using the normalize=True argument gives us the index-wise proportion of the data
    cross_tab_prop = pd.crosstab(index=df[col], columns=df['target'], normalize='index')

    # Define colormap
    cmp = ListedColormap(['#ff826e', 'red'])

    # Plot stacked bar charts
    x, y = i//4, i%4
    cross_tab_prop.plot(kind='bar', ax=ax[x,y], stacked=True, width=0.8, colormap=cmp,
                        legend=False, ylabel='Proportion', sharey=True)

    # Add the proportions and counts of the individual bars to our plot
    for idx, val in enumerate([*cross_tab.index.values]):
        for (proportion, count, y_location) in zip(cross_tab_prop.loc[val],cross_tab.loc[val],cross_tab_prop.loc[val].cumsum()):
            ax[x,y].text(x=idx-0.3, y=(y_location-proportion)+(proportion/2)-0.03,
                         s = f'    {count}\n({np.round(proportion * 100, 1)}%)',
                         color = "black", fontsize=9, fontweight="bold")

    # Add legend
    ax[x,y].legend(title='target', loc=(0.7,0.9), fontsize=8, ncol=2)
    # Set y limit
    ax[x,y].set_ylim([0,1.12])
    # Rotate xticks
    ax[x,y].set_xticklabels(ax[x,y].get_xticklabels(), rotation=0)


plt.suptitle('Categorical Features vs Target Stacked Barplots', fontsize=22)
plt.tight_layout()
plt.show()

"""Data Preprocessing

Missing Value
"""

# Check for missing values in the dataset
df.isnull().sum().sum()

"""**Outlier Treatment**

check for outliers using the IQR method for the continuous features
"""

continuous_features

import seaborn as sns
import matplotlib.pyplot as plt

# Set up the figure
plt.figure(figsize=(12, 6))

# Create boxplots for each continuous feature
for i, feature in enumerate(continuous_features, 1):
    plt.subplot(2, 3, i)  # 2 rows, 3 columns layout

    # Create boxplot
    boxplot = sns.boxplot(x=df[feature], color='skyblue', width=0.4)

    # Calculate outliers using IQR
    Q1 = df[feature].quantile(0.25)
    Q3 = df[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Highlight outliers
    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]
    if not outliers.empty:
        plt.scatter(outliers[feature], [0]*len(outliers), color='red', alpha=0.6,
                   label=f'Outliers ({len(outliers)})')

    # Add titles and labels
    plt.title(f'{feature}\n(IQR: {IQR:.1f})', fontsize=10)
    plt.xlabel('')
    plt.xticks(rotation=45)

    # Add legend if there are outliers
    if not outliers.empty:
        plt.legend(loc='upper right', fontsize=8)

plt.suptitle('Outlier Detection in Continuous Features (Using IQR Method)', y=1.02, fontsize=12)
plt.tight_layout()
plt.show()

# Print the count of outliers for each feature
print("Number of outliers detected in each feature:")
print(outliers_count_specified)

"""Duplicate Rows"""

# Identify duplicate rows and keep the first occurrence
duplicates = df[df.duplicated(keep='first')]

# Find the index of the original row each duplicate corresponds to
duplicates_with_original = df[df.duplicated(keep=False)]

# Display the duplicate rows along with the indices of the original rows
duplicates_with_original['original_row'] = duplicates_with_original.duplicated(keep='first').cumsum()

# Display duplicate rows with corresponding original row index
print(duplicates_with_original)

# Remove duplicate rows
df_cleaned = df.drop_duplicates()
print(f"Dataset after removing duplicates: {df_cleaned.shape}")

"""Outlier Removal"""

# Function to remove outliers using IQR method
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Remove outliers from selected numerical columns
for col in ["trestbps", "chol", "oldpeak"]:
    df_cleaned = remove_outliers(df_cleaned, col)

# Verify the new dataset size after outlier removal
print(f"Dataset after removing outliers: {df_cleaned.shape}")

"""Scaling Features"""

from sklearn.preprocessing import StandardScaler

# Standardizing numerical features
scaler = StandardScaler()

# Separate features and target variable
X = df_cleaned.drop(columns=["target"])
y = df_cleaned["target"].astype(int)

# Apply scaling to the features
X_scaled = scaler.fit_transform(X)

# Convert scaled features back to a DataFrame
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Now X_scaled can be used for model training

"""Train-Test Split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")

from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, cross_val_predict
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Define Stratified K-Fold for cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# ---- BASE MODEL (No Hyperparameter Tuning) ----
base_model = GaussianNB()
base_scores = cross_val_score(base_model, X_scaled, y, cv=cv, scoring='accuracy')
base_predictions = cross_val_predict(base_model, X_scaled, y, cv=cv)

# Fit the base model to ensure classes_ attribute is available
base_model.fit(X_scaled, y)

print(f"Base Naïve Bayes Model Accuracy (Cross-Validation): {np.mean(base_scores):.4f}")
print("Base Model Classification Report:")
print(classification_report(y, base_predictions))

# ========== ADDED FEATURE IMPORTANCE VISUALIZATION ==========
# Get feature means per class from the GaussianNB model
class_0_means = base_model.theta_[0]  # Mean values for class 0
class_1_means = base_model.theta_[1]  # Mean values for class 1
feature_importance = np.abs(class_1_means - class_0_means)  # Absolute difference between class means

# Create and display feature importance table
importance_df = pd.DataFrame({
    'Feature': X_scaled.columns,
    'Class_0_Mean': class_0_means,
    'Class_1_Mean': class_1_means,
    'Importance': feature_importance
}).sort_values('Importance', ascending=False)

print("\nFeature Importance Table:")
print(importance_df.to_markdown(tablefmt="grid", floatfmt=".3f"))

# Plot feature importance
plt.figure(figsize=(10, 6))
bars = plt.barh(importance_df['Feature'], importance_df['Importance'], color='dodgerblue')
plt.xlabel('Importance (Absolute Mean Difference)')
plt.title('GaussianNB Feature Importance')
plt.gca().invert_yaxis()  # Highest importance at top
plt.grid(axis='x', alpha=0.3)

# Add value labels to bars
for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,
             f'{width:.2f}',
             va='center', ha='left')

plt.tight_layout()
plt.show()
# ========== END OF ADDED CODE ==========

# ---- [YOUR ORIGINAL CODE CONTINUES BELOW WITHOUT ANY CHANGES] ----
# ---- CUSTOMIZED MODEL (Hyperparameter Tuning) ----
# Define parameter grid for Naïve Bayes
param_grid_gaussian = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3],  # test more smoothing values for GaussianNB
}

param_grid_bernoulli_multinomial = {
    'alpha': [1, 0.1, 0.01, 0.001],  # Smoothing parameter for BernoulliNB and MultinomialNB
    'class_prior': [None, [0.4, 0.6], [0.3, 0.7]]  # Prior probabilities for class imbalance
}

# Initialize models to tune
models = {
    'GaussianNB': GaussianNB(),
    'BernoulliNB': BernoulliNB(),
    'MultinomialNB': MultinomialNB()
}

best_model_name = None
best_model_score = 0
best_model = None

# Apply separate scaling for MultinomialNB to avoid negative values
for model_name, model in models.items():
    print(f"Training {model_name} with GridSearchCV...")

    if model_name == 'GaussianNB':
        grid_search = GridSearchCV(model, param_grid_gaussian, cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_scaled, y)  # Fit GaussianNB using StandardScaler data
    elif model_name == 'MultinomialNB':
        # For MultinomialNB, apply MinMaxScaler to original features to avoid negative values
        min_max_scaler = MinMaxScaler()
        X_scaled_multinomial = min_max_scaler.fit_transform(df_cleaned[continuous_features])  # Use only original features
        grid_search = GridSearchCV(model, param_grid_bernoulli_multinomial, cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_scaled_multinomial, y)  # Fit using MinMax-scaled data for MultinomialNB
    else:  # For BernoulliNB, use StandardScaler
        grid_search = GridSearchCV(model, param_grid_bernoulli_multinomial, cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_scaled, y) # Fit BernoulliNB using StandardScaler data

    best_score = grid_search.best_score_
    best_params = grid_search.best_params_
    best_model_candidate = grid_search.best_estimator_

    print(f"Best {model_name} Accuracy (GridSearchCV): {best_score:.4f}")
    print(f"Best Parameters: {best_params}")

    if best_score > best_model_score:
        best_model_score = best_score
        best_model_name = model_name
        best_model = best_model_candidate

print(f"Best Tuned Model: {best_model_name} with Accuracy: {best_model_score:.4f}")

# Predict using the best model
best_model_predictions = cross_val_predict(best_model, X_scaled, y, cv=cv)
print("Best Tuned Model Classification Report:")
print(classification_report(y, best_model_predictions))

# Generate confusion matrices for both models
conf_matrix_base = confusion_matrix(y, base_predictions)
conf_matrix_custom = confusion_matrix(y, best_model_predictions)

# Plot confusion matrices side by side
fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # Adjust figsize to make plots smaller but not too small

# Base Model Confusion Matrix
disp_base = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_base, display_labels=base_model.classes_)
disp_base.plot(ax=axes[0], cmap=plt.cm.Blues)
axes[0].set_title("Confusion Matrix - Base Model")

# Customised Model Confusion Matrix
disp_custom = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_custom, display_labels=best_model.classes_)
disp_custom.plot(ax=axes[1], cmap=plt.cm.Blues)
axes[1].set_title("Confusion Matrix - Customised Model")

plt.tight_layout()
plt.show()

# ---- RESULT COMPARISON ----
print("\nComparison:")
print(f"Base Model Accuracy: {np.mean(base_scores):.4f}")
print(f"Best Tuned Model Accuracy: {best_model_score:.4f}")

from sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold, cross_val_predict
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Define Stratified K-Fold for cross-validation
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# ---- BASE MODEL (No Hyperparameter Tuning) ----
base_model = LogisticRegression(max_iter=2000, class_weight='balanced')
base_scores = cross_val_score(base_model, X_scaled, y, cv=cv, scoring='accuracy')
base_predictions = cross_val_predict(base_model, X_scaled, y, cv=cv)

# Fit the base model to ensure classes_ attribute is available
base_model.fit(X_scaled, y)

print(f"Base Logistic Regression Model Accuracy (Cross-Validation): {np.mean(base_scores):.4f}")
print("Base Model Classification Report:")
print(classification_report(y, base_predictions))

# ========== ADDED FEATURE IMPORTANCE VISUALIZATION ==========
# Get coefficients (absolute values for importance)
coefficients = np.abs(base_model.coef_[0])
feature_names = X_scaled.columns

# Create importance DataFrame
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': base_model.coef_[0],
    'Absolute_Importance': coefficients
}).sort_values('Absolute_Importance', ascending=False)

# Display the table
print("\nFeature Importance Table:")
print(importance_df.to_markdown(tablefmt="grid", floatfmt=".3f"))

# Plot feature importance
plt.figure(figsize=(10, 6))
bars = plt.barh(importance_df['Feature'], importance_df['Absolute_Importance'], color='purple')
plt.xlabel('Absolute Coefficient Value')
plt.title('Logistic Regression Feature Importance')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)

# Add value labels
for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,
             f'{width:.3f}',
             va='center', ha='left')

plt.tight_layout()
plt.show()
# ========== END OF ADDED CODE ==========

# ---- [YOUR ORIGINAL CODE CONTINUES BELOW WITHOUT ANY CHANGES] ----
# ---- FEATURE SELECTION ----
selector = SelectKBest(score_func=f_classif, k=20)
X_selected = selector.fit_transform(X_scaled, y)

# ---- CUSTOMIZED MODEL (Hyperparameter Tuning) ----
param_grid_logreg = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'lbfgs', 'saga'],
    'penalty': ['l1', 'l2'],
    'max_iter': [1000, 2000]
}

logreg_model = LogisticRegression(class_weight='balanced')

random_search_logreg = RandomizedSearchCV(
    logreg_model, param_grid_logreg, cv=cv, scoring='accuracy', n_jobs=-1, n_iter=50, random_state=42
)
random_search_logreg.fit(X_selected, y)

best_logreg_score = random_search_logreg.best_score_
best_logreg_params = random_search_logreg.best_params_
best_logreg_model = random_search_logreg.best_estimator_

print(f"Best Logistic Regression Accuracy (RandomizedSearchCV): {best_logreg_score:.4f}")
print(f"Best Parameters: {best_logreg_params}")

best_logreg_predictions = cross_val_predict(best_logreg_model, X_selected, y, cv=cv)
print("Best Tuned Logistic Regression Model Classification Report:")
print(classification_report(y, best_logreg_predictions))

# ---- Confusion Matrices ----
conf_matrix_base = confusion_matrix(y, base_predictions)
conf_matrix_best = confusion_matrix(y, best_logreg_predictions)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
disp_base = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_base, display_labels=base_model.classes_)
disp_base.plot(ax=axes[0], cmap=plt.cm.Blues)
axes[0].set_title("Confusion Matrix - Base Model")

disp_best = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_best, display_labels=best_logreg_model.classes_)
disp_best.plot(ax=axes[1], cmap=plt.cm.Blues)
axes[1].set_title("Confusion Matrix - Best Tuned Model")

plt.tight_layout()
plt.show()

# ---- RESULT COMPARISON ----
print("\nComparison:")
print(f"Base Logistic Regression Model Accuracy: {np.mean(base_scores):.4f}")
print(f"Best Tuned Logistic Regression Model Accuracy: {best_logreg_score:.4f}")

# ========== ADDED BEST MODEL FEATURE IMPORTANCE ==========
if hasattr(best_logreg_model, 'coef_'):
    print("\n=== Best Model Feature Importance ===")
    best_coefficients = np.abs(best_logreg_model.coef_[0])

    best_importance_df = pd.DataFrame({
        'Feature': feature_names[selector.get_support()],  # Only show selected features
        'Coefficient': best_logreg_model.coef_[0],
        'Absolute_Importance': best_coefficients
    }).sort_values('Absolute_Importance', ascending=False)

    print("\nBest Model Feature Importance Table:")
    print(best_importance_df.to_markdown(tablefmt="grid", floatfmt=".3f"))

    plt.figure(figsize=(10, 6))
    bars = plt.barh(best_importance_df['Feature'], best_importance_df['Absolute_Importance'], color='darkorange')
    plt.xlabel('Absolute Coefficient Value')
    plt.title('Best Logistic Regression Feature Importance (Selected Features)')
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)

    for bar in bars:
        width = bar.get_width()
        plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,
                 f'{width:.3f}',
                 va='center', ha='left')

    plt.tight_layout()
    plt.show()

from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, cross_val_predict
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Define Stratified K-Fold for cross-validation
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# ---- BASE MODEL (No Hyperparameter Tuning) ----
base_model = DecisionTreeClassifier(random_state=42)
base_scores = cross_val_score(base_model, X_scaled, y, cv=cv, scoring='accuracy')
base_predictions = cross_val_predict(base_model, X_scaled, y, cv=cv)

# Fit the base model to ensure classes_ attribute is available
base_model.fit(X_scaled, y)

print(f"Base Decision Tree Classifier Accuracy (Cross-Validation): {np.mean(base_scores):.4f}")
print("Base Model Classification Report:")
print(classification_report(y, base_predictions))

# ========== ADDED FEATURE IMPORTANCE VISUALIZATION ==========
# Get feature importances
importances = base_model.feature_importances_
feature_names = X_scaled.columns

# Create importance DataFrame
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values('Importance', ascending=False)

# Display the table
print("\nFeature Importance Table:")
print(importance_df.to_markdown(tablefmt="grid", floatfmt=".4f"))

# Plot feature importance
plt.figure(figsize=(10, 6))
bars = plt.barh(importance_df['Feature'], importance_df['Importance'], color='green')
plt.xlabel('Feature Importance Score')
plt.title('Decision Tree Feature Importance')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)

# Add value labels
for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.005, bar.get_y() + bar.get_height()/2,
             f'{width:.4f}',
             va='center', ha='left')

plt.tight_layout()
plt.show()
# ========== END OF ADDED CODE ==========

# ---- [YOUR ORIGINAL CODE CONTINUES BELOW WITHOUT ANY CHANGES] ----
# ---- CUSTOMIZED MODEL (Hyperparameter Tuning) ----
param_grid_dt = {
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'max_features': [None, 'sqrt', 'log2'],
    'criterion': ['gini', 'entropy']
}

dt_model = DecisionTreeClassifier(random_state=42)

grid_search_dt = GridSearchCV(dt_model, param_grid_dt, cv=10, scoring='accuracy', n_jobs=-1)
grid_search_dt.fit(X_scaled, y)

best_dt_score = grid_search_dt.best_score_
best_dt_params = grid_search_dt.best_params_
best_dt_model = grid_search_dt.best_estimator_

print(f"Best Decision Tree Classifier Accuracy (GridSearchCV): {best_dt_score:.4f}")
print(f"Best Parameters: {best_dt_params}")

best_dt_predictions = cross_val_predict(best_dt_model, X_scaled, y, cv=cv)
print("Best Tuned Decision Tree Classifier Classification Report:")
print(classification_report(y, best_dt_predictions))

# ---- Confusion Matrices ----
conf_matrix_base = confusion_matrix(y, base_predictions)
conf_matrix_best = confusion_matrix(y, best_dt_predictions)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
disp_base = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_base, display_labels=base_model.classes_)
disp_base.plot(ax=axes[0], cmap=plt.cm.Blues)
axes[0].set_title("Confusion Matrix - Base Model")

disp_best = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_best, display_labels=best_dt_model.classes_)
disp_best.plot(ax=axes[1], cmap=plt.cm.Blues)
axes[1].set_title("Confusion Matrix - Best Tuned Model")

plt.tight_layout()
plt.show()

# ---- RESULT COMPARISON ----
print("\nComparison:")
print(f"Base Decision Tree Classifier Accuracy: {np.mean(base_scores):.4f}")
print(f"Best Tuned Decision Tree Classifier Accuracy: {best_dt_score:.4f}")

# ========== ADDED BEST MODEL FEATURE IMPORTANCE ==========
print("\n=== Best Model Feature Importance ===")
best_importances = best_dt_model.feature_importances_

best_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': best_importances
}).sort_values('Importance', ascending=False)

print("\nBest Model Feature Importance Table:")
print(best_importance_df.to_markdown(tablefmt="grid", floatfmt=".4f"))

plt.figure(figsize=(10, 6))
bars = plt.barh(best_importance_df['Feature'], best_importance_df['Importance'], color='darkgreen')
plt.xlabel('Feature Importance Score')
plt.title('Best Decision Tree Feature Importance')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)

for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.005, bar.get_y() + bar.get_height()/2,
             f'{width:.4f}',
             va='center', ha='left')

plt.tight_layout()
plt.show()

from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, cross_val_predict
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.inspection import permutation_importance

# Define Stratified K-Fold for cross-validation
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# ---- BASE MODEL (No Hyperparameter Tuning) ----
base_model = SVC(random_state=42)
base_scores = cross_val_score(base_model, X_scaled, y, cv=cv, scoring='accuracy')
base_predictions = cross_val_predict(base_model, X_scaled, y, cv=cv)

# Fit the base model to ensure classes_ attribute is available
base_model.fit(X_scaled, y)

print(f"Base SVM Model Accuracy (Cross-Validation): {np.mean(base_scores):.4f}")
print("Base Model Classification Report:")
print(classification_report(y, base_predictions))

# ========== ADDED FEATURE IMPORTANCE VISUALIZATION ==========
# For SVM with linear kernel, we can use coefficients directly
if base_model.kernel == 'linear':
    coef = base_model.coef_[0]
    importance = np.abs(coef)
    feature_names = X_scaled.columns

    # Create importance DataFrame
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': coef,
        'Absolute_Importance': importance
    }).sort_values('Absolute_Importance', ascending=False)

    print("\nFeature Importance Table (Linear Kernel Coefficients):")
    print(importance_df.to_markdown(tablefmt="grid", floatfmt=".4f"))

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    bars = plt.barh(importance_df['Feature'], importance_df['Absolute_Importance'], color='purple')
    plt.xlabel('Absolute Coefficient Value')
    plt.title('SVM Feature Importance (Linear Kernel)')
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)

    # Add value labels
    for bar in bars:
        width = bar.get_width()
        plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,
                 f'{width:.4f}',
                 va='center', ha='left')

    plt.tight_layout()
    plt.show()
else:
    # For non-linear kernels, use permutation importance
    print("\nCalculating Permutation Importance (this may take a few minutes)...")
    perm_importance = permutation_importance(
        base_model, X_scaled, y, n_repeats=10, random_state=42, n_jobs=-1
    )

    # Create importance DataFrame
    importance_df = pd.DataFrame({
        'Feature': X_scaled.columns,
        'Importance': perm_importance.importances_mean,
        'Std': perm_importance.importances_std
    }).sort_values('Importance', ascending=False)

    print("\nFeature Importance Table (Permutation Importance):")
    print(importance_df.to_markdown(tablefmt="grid", floatfmt=".4f"))

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    y_pos = np.arange(len(importance_df))
    plt.barh(y_pos, importance_df['Importance'], xerr=importance_df['Std'],
             color='orange', ecolor='black', capsize=5)
    plt.yticks(y_pos, importance_df['Feature'])
    plt.xlabel('Permutation Importance Score')
    plt.title('SVM Feature Importance (Permutation)')
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.show()
# ========== END OF ADDED CODE ==========

# ---- [YOUR ORIGINAL CODE CONTINUES BELOW WITHOUT ANY CHANGES] ----
# ---- CUSTOMIZED MODEL (Hyperparameter Tuning) ----
param_grid_svm = {
    'C': [0.1, 1, 10, 100, 1000],
    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],
    'gamma': ['scale', 'auto'],
    'degree': [3, 4, 5]
}

svm_model = SVC(random_state=42)

grid_search_svm = GridSearchCV(svm_model, param_grid_svm, cv=10, scoring='accuracy', n_jobs=-1)
grid_search_svm.fit(X_scaled, y)

best_svm_score = grid_search_svm.best_score_
best_svm_params = grid_search_svm.best_params_
best_svm_model = grid_search_svm.best_estimator_

print(f"Best SVM Accuracy (GridSearchCV): {best_svm_score:.4f}")
print(f"Best Parameters: {best_svm_params}")

best_svm_predictions = cross_val_predict(best_svm_model, X_scaled, y, cv=cv)
print("Best Tuned SVM Model Classification Report:")
print(classification_report(y, best_svm_predictions))

# ---- Confusion Matrices ----
conf_matrix_base = confusion_matrix(y, base_predictions)
conf_matrix_best = confusion_matrix(y, best_svm_predictions)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
disp_base = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_base, display_labels=base_model.classes_)
disp_base.plot(ax=axes[0], cmap=plt.cm.Blues)
axes[0].set_title("Confusion Matrix - Base Model")

disp_best = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_best, display_labels=best_svm_model.classes_)
disp_best.plot(ax=axes[1], cmap=plt.cm.Blues)
axes[1].set_title("Confusion Matrix - Best Tuned Model")

plt.tight_layout()
plt.show()

# ---- RESULT COMPARISON ----
print("\nComparison:")
print(f"Base SVM Model Accuracy: {np.mean(base_scores):.4f}")
print(f"Best Tuned SVM Model Accuracy: {best_svm_score:.4f}")

# ========== ADDED BEST MODEL FEATURE IMPORTANCE ==========
if best_svm_model.kernel == 'linear':
    print("\n=== Best Model Feature Importance ===")
    coef = best_svm_model.coef_[0]
    importance = np.abs(coef)

    best_importance_df = pd.DataFrame({
        'Feature': X_scaled.columns,
        'Coefficient': coef,
        'Absolute_Importance': importance
    }).sort_values('Absolute_Importance', ascending=False)

    print("\nBest Model Feature Importance Table:")
    print(best_importance_df.to_markdown(tablefmt="grid", floatfmt=".4f"))

    plt.figure(figsize=(10, 6))
    bars = plt.barh(best_importance_df['Feature'], best_importance_df['Absolute_Importance'], color='darkviolet')
    plt.xlabel('Absolute Coefficient Value')
    plt.title('Best SVM Feature Importance (Linear Kernel)')
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)

    for bar in bars:
        width = bar.get_width()
        plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,
                 f'{width:.4f}',
                 va='center', ha='left')

    plt.tight_layout()
    plt.show()
else:
    print("\nCalculating Best Model Permutation Importance...")
    perm_importance = permutation_importance(
        best_svm_model, X_scaled, y, n_repeats=10, random_state=42, n_jobs=-1
    )

    best_importance_df = pd.DataFrame({
        'Feature': X_scaled.columns,
        'Importance': perm_importance.importances_mean,
        'Std': perm_importance.importances_std
    }).sort_values('Importance', ascending=False)

    print("\nBest Model Feature Importance Table:")
    print(best_importance_df.to_markdown(tablefmt="grid", floatfmt=".4f"))

    plt.figure(figsize=(10, 6))
    y_pos = np.arange(len(best_importance_df))
    plt.barh(y_pos, best_importance_df['Importance'], xerr=best_importance_df['Std'],
             color='red', ecolor='black', capsize=5)
    plt.yticks(y_pos, best_importance_df['Feature'])
    plt.xlabel('Permutation Importance Score')
    plt.title('Best SVM Feature Importance (Permutation)')
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.show()

from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, cross_val_predict
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Define Stratified K-Fold for cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Check if X_scaled and y have the same number of samples
print(f"Shape of X_scaled: {X_scaled.shape}")
print(f"Shape of y: {y.shape}")

# Ensure X_scaled and y have the same number of samples
assert X_scaled.shape[0] == y.shape[0], "Mismatch between X and y!"

# ---- BASE MODEL (No Hyperparameter Tuning) ----
base_model = RandomForestClassifier(random_state=42)
base_scores = cross_val_score(base_model, X_scaled, y, cv=cv, scoring='accuracy')
base_predictions = cross_val_predict(base_model, X_scaled, y, cv=cv)

# Fit the base model to ensure classes_ attribute is available
base_model.fit(X_scaled, y)

print(f"Base Random Forest Classifier Accuracy (Cross-Validation): {np.mean(base_scores):.4f}")
print("Base Model Classification Report:")
print(classification_report(y, base_predictions))

# ========== ADDED FEATURE IMPORTANCE VISUALIZATION ==========
# Get feature importances
importances = base_model.feature_importances_
feature_names = X_scaled.columns
std = np.std([tree.feature_importances_ for tree in base_model.estimators_], axis=0)

# Create importance DataFrame
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances,
    'Std': std
}).sort_values('Importance', ascending=False)

# Display the table
print("\nFeature Importance Table:")
print(importance_df.to_markdown(tablefmt="grid", floatfmt=".4f"))

# Plot feature importance
plt.figure(figsize=(12, 6))
bars = plt.barh(importance_df['Feature'], importance_df['Importance'],
               xerr=importance_df['Std'], color='forestgreen', ecolor='black', capsize=5)
plt.xlabel('Mean Decrease in Impurity', fontsize=12)
plt.title('Random Forest Feature Importance (Base Model)', fontsize=14)
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)

# Add value labels
for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.005, bar.get_y() + bar.get_height()/2,
             f'{width:.4f}',
             va='center', ha='left')

plt.tight_layout()
plt.show()
# ========== END OF ADDED CODE ==========

# ---- [YOUR ORIGINAL CODE CONTINUES BELOW WITHOUT ANY CHANGES] ----
# ---- CUSTOMIZED MODEL (Hyperparameter Tuning) ----
param_grid_rf = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'criterion': ['gini', 'entropy']
}

rf_model = RandomForestClassifier(random_state=42)

grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_rf.fit(X_scaled, y)

best_rf_score = grid_search_rf.best_score_
best_rf_params = grid_search_rf.best_params_
best_rf_model = grid_search_rf.best_estimator_

print(f"Best Random Forest Classifier Accuracy (GridSearchCV): {best_rf_score:.4f}")
print(f"Best Parameters: {best_rf_params}")

best_rf_predictions = cross_val_predict(best_rf_model, X_scaled, y, cv=cv)
print("Best Tuned Random Forest Classifier Classification Report:")
print(classification_report(y, best_rf_predictions))

# ---- Confusion Matrices ----
conf_matrix_base = confusion_matrix(y, base_predictions)
conf_matrix_best = confusion_matrix(y, best_rf_predictions)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
disp_base = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_base, display_labels=base_model.classes_)
disp_base.plot(ax=axes[0], cmap=plt.cm.Blues)
axes[0].set_title("Confusion Matrix - Base Model")

disp_best = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_best, display_labels=best_rf_model.classes_)
disp_best.plot(ax=axes[1], cmap=plt.cm.Blues)
axes[1].set_title("Confusion Matrix - Best Tuned Model")

plt.tight_layout()
plt.show()

# ---- RESULT COMPARISON ----
print("\nComparison:")
print(f"Base Random Forest Classifier Accuracy: {np.mean(base_scores):.4f}")
print(f"Best Tuned Random Forest Classifier Accuracy: {best_rf_score:.4f}")

# ========== ADDED BEST MODEL FEATURE IMPORTANCE ==========
print("\n=== Best Model Feature Importance ===")
best_importances = best_rf_model.feature_importances_
best_std = np.std([tree.feature_importances_ for tree in best_rf_model.estimators_], axis=0)

best_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': best_importances,
    'Std': best_std
}).sort_values('Importance', ascending=False)

print("\nBest Model Feature Importance Table:")
print(best_importance_df.to_markdown(tablefmt="grid", floatfmt=".4f"))

plt.figure(figsize=(12, 6))
bars = plt.barh(best_importance_df['Feature'], best_importance_df['Importance'],
               xerr=best_importance_df['Std'], color='darkgreen', ecolor='black', capsize=5)
plt.xlabel('Mean Decrease in Impurity', fontsize=12)
plt.title('Random Forest Feature Importance (Best Tuned Model)', fontsize=14)
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)

for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.005, bar.get_y() + bar.get_height()/2,
             f'{width:.4f}',
             va='center', ha='left')

plt.tight_layout()
plt.show()

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Set very compact default figure size
plt.rcParams['figure.figsize'] = (4, 3)  # Width, Height in inches
plt.rcParams['font.size'] = 8  # Smaller font size
plt.rcParams['axes.titlepad'] = 6  # Reduce title padding

# AdaBoost Implementation
print("=== AdaBoost Classifier ===")
ada = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=2),
    n_estimators=200,
    learning_rate=0.5,
    random_state=42
)

# Train and evaluate
ada.fit(X_scaled, y)
ada_pred = ada.predict(X_scaled)
ada_acc = accuracy_score(y, ada_pred)

print(f"Accuracy: {ada_acc:.4f}")
print("Classification Report:")
print(classification_report(y, ada_pred))

# Feature Importance Table
ada_importance = pd.DataFrame({
    'Feature': X_scaled.columns,
    'Importance': ada.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nAdaBoost Feature Importance Table:")
print(ada_importance.to_markdown(tablefmt="grid", index=False, floatfmt=".4f"))

# Confusion Matrix - Very compact
plt.figure(figsize=(3.5, 2.8))
cm = confusion_matrix(y, ada_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Healthy', 'Heart Disease'])
disp.plot(cmap='Blues')
plt.title('AdaBoost CM', fontsize=9)
plt.tight_layout()
plt.show()

# Feature Importance Plot - Very compact
plt.figure(figsize=(3.5, 2.5))
bars = ada_importance.sort_values('Importance').plot.barh(
    x='Feature', y='Importance',
    color='royalblue', legend=False
)
plt.title('AdaBoost Features', fontsize=9)

# Add value labels to bars
for i, (_, row) in enumerate(ada_importance.sort_values('Importance').iterrows()):
    plt.text(row['Importance'] + 0.005, i,
             f"{row['Importance']:.3f}",
             va='center', ha='left', fontsize=7)

plt.tight_layout()
plt.show()

# Final Results
print("\n=== Final Accuracy Score ===")
print(f"AdaBoost: {ada_acc:.4f}")